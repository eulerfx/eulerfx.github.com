<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: philosophy | Lev Gorodinski]]></title>
  <link href="http://gorodinski.com/blog/categories/philosophy/atom.xml" rel="self"/>
  <link href="http://gorodinski.com/"/>
  <updated>2014-07-21T20:33:50-04:00</updated>
  <id>http://gorodinski.com/</id>
  <author>
    <name><![CDATA[Lev Gorodinski]]></name>
    <email><![CDATA[eulerfx@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Placing knowledge on center stage]]></title>
    <link href="http://gorodinski.com/blog/2012/12/10/placing-knowledge-on-center-stage/"/>
    <updated>2012-12-10T21:22:00-05:00</updated>
    <id>http://gorodinski.com/blog/2012/12/10/placing-knowledge-on-center-stage</id>
    <content type="html"><![CDATA[<p><em>In this post I survey the evolution of various programming paradigms and emphasize the importance of expressing and isolating domain knowledge. Paradigms such as OOP, AOP, DCI, DDD, and Hexagonal are regarded as having a central goal of facilitating the representation of knowledge all while supporting integration with technical components. Finally I introduce a knowledge-driven architecture by Jeff Zhuk.</em></p>

<!--more-->


<p>The vast majority of computing today can be decomposed into operations of a <a href="http://en.wikipedia.org/wiki/Turing_machine">Turing Machine</a>. Contrarily, the vast majority of humans think in terms of concepts far beyond symbols on a tape. Perhaps, as alluded to by <a href="http://en.wikipedia.org/wiki/Douglas_Hofstadter">Douglas Hofstadter</a> in <a href="http://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">Godel, Escher, Bach</a>, consciousness is merely an illusion established by a balance between self-knowledge and self-ignorance. Self-knowledge is the extent to which we are aware of our thoughts and are able to trace the actions of our mind. Self-ignorance consists of the sub-conscious as well as all of the functions of the <a href="http://en.wikipedia.org/wiki/Central_nervous_system">central nervous system</a>. Given a thought, we can likely factor it into constituent propositions and statements, which themselves may be further factored. On the other hand, we can't feel the firing of the underlying neurons or operations of the <a href="http://en.wikipedia.org/wiki/Cerebral_cortex">cerebral cortex</a>.</p>

<p>Natural characteristics of the brain and mind are in turn reflected in the architectures of computing devices. The <a href="http://en.wikipedia.org/wiki/Central_processing_unit">CPU</a> performs very basic arithmetical and logical operations the fundamental principals of which have remained unchanged since its inception. The software which the CPU ultimately runs however is far more complex than those basic operations. Programming languages and the practice of software engineering have been devised to tame this dichotomy. Yet today, many years after the first CPU and the first program, there remains an ongoing battle between the lower-level forces of hardware and the higher-level forces of domain knowledge.</p>

<h2>Imperative &amp; Declarative</h2>

<p>The battle between man and machine is fittingly illustrated by the contrast between <a href="http://en.wikipedia.org/wiki/Imperative_programming">imperative languages</a> and <a href="http://en.wikipedia.org/wiki/Declarative_programming">declarative languages</a>. Imperative languages can be thought of as bottom-up <a href="http://gorodinski.com/blog/2012/05/31/abstractions/">abstractions</a> over the underlying hardware. Declarative languages on the other hand are top-down - they represent information and leave it up to the language <a href="http://en.wikipedia.org/wiki/Compiler">compiler</a> to translate and convey this information to the underlying hardware. Functional languages in particular are declarative because they are implementations of the <a href="http://en.wikipedia.org/wiki/Lambda_calculus">lambda calculus</a> on a Turing machine. In terms of practical utility, imperative languages have been winning the battle as evidenced by the <a href="http://langpop.com/">predominance of C</a> decades after its creation. The bare-bones simplicity of C and its proximity to the underlying machine are <a href="http://programmers.stackexchange.com/questions/141329/what-makes-c-so-popular-in-the-age-of-oop">part of the reason</a> for its continual relevance. What this indicates, however, is that programming language technology has yet to attain the level of abstraction and expressive power to make something like C less relevant.</p>

<h2>Object-oriented</h2>

<p>Regardless of the continual prevalence of lower level languages, ambitious attempts at elevating abstraction can provide valuable insight. Take for instance <a href="http://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented programming</a>. <a href="http://en.wikipedia.org/wiki/Douglas_Engelbart">Douglas Engelbart</a> envisioned the computer as an extension of the human mind and OOP can be regarded as the incarnation of his vision. Today, OOP is a predominant programming paradigm. The problem is that the promise of object's capacity to capture the end user's mental can be deceptive. In the context of GUIs objects serve well in representing the domain. However, for other domains, especially ones based on reality such as <a href="http://en.wikipedia.org/wiki/Line_of_business">LOB</a> applications, OOP's weaknesses in expressing collaboration can become a notable design and modeling hindrance. OOP can also be somewhat misleading because a class can rarely represent its counterpart in reality to the full extent. For example, a bank account class in an ATM application may model state to represent the available balance and expose behavior for adjusting the balance while protecting invariants. This however represents a small fraction of the functionality required to perform a withdrawal, which also entails aspects such as transactions, server connections, etc. The ATM withdrawal example is drawn from an <a href="http://www.artima.com/articles/dci_vision.html">article on DCI architecture</a> which provides a framework for expressing collaborations between objects based on roles.</p>

<h2>DCI, Hexagonal and Domain-Driven</h2>

<p>The method of action of the DCI architecture facilitates explicit representation of domain knowledge by providing a tailored language of expression as an OOP based framework. DCI was devised in order to compensate for the lack of behavioral expressiveness in traditional OOP. Not surprisingly, similar instances of domain knowledge emphasis abound. An age old mantra in software engineering is the segregation of business logic from presentation and infrastructure logic. This segregation is beneficial not only due to advantages of traditional layering but also due to the emergent isolation of domain knowledge. Alistair Cockburn's <a href="http://alistair.cockburn.us/Hexagonal+architecture">Hexagonal Architecture</a> builds upon this idea and applies it at an architectural level. Domain knowledge is placed at the center with infrastructure components <em>adapting to</em> it. In a sense, knowledge "ripples" from the core throughout components which integrate this knowledge with infrastructure. Another prominent example of knowledge isolation is <a href="http://stackoverflow.com/tags/domain-driven-design/info">Domain-Driven Design</a>. A fundamental premise of DDD is placing focus on the core domain, on domain knowledge. The intent is to capture the informational core of the business problem. The remaining components of a working system, while being absolutely essential, are supporting in nature. In retrospect, all of this makes a great deal of sense - after all, computers were designed to solve human problems, not the other way around.</p>

<h2>Aspect-oriented</h2>

<p><a href="http://en.wikipedia.org/wiki/Aspect-oriented_programming">Aspect-oriented programming</a> introduces new mechanisms of composition, partitioning and encapsulation through the notion of a <em>concern</em>. Concerns contain pieces of domain knowledge and the facilities provided by AOP enable composition of concerns and associated behaviors. As a whole, the aspect-oriented paradigm establishes an informational topology wherein knowledge propagates from the core domain out to supporting components. Much like the other paradigms, this type of topology is effective due to its positioning of domain knowledge at the center.</p>

<h2>Rediscovering the I in IT</h2>

<p>Despite significant advances in programming language theory and software architecture, the I in <a href="http://en.wikipedia.org/wiki/Information_technology">IT</a> is all too often overshadowed by the T. <a href="https://twitter.com/raganwald">Reg Braithwaite</a> portrays this phenomenon in <a href="http://raganwald.com/2006/12/economizing-can-be-penny-wise-and.html">Economizing can be penny-wise and pound foolish</a> by coloring code to depict the signal to noise ratio. Green colored code is code that directly express the problem at hand. Yellow colored code represents the accidental complexity of a programming language. Red represents code which has no identifiable function. The goal then is to eliminate red code, reduce yellow code and emphasize the green code.</p>

<p>How can we get there? Where are the weakest links? To some extent the issue is driven by the fact that programming languages carry a double burden. On one hand, a programming language is a place to organize one's thoughts and express domain knowledge. On the other hand, a programming language must be compiled or interpreted to be ultimately converted into a series of elementary memory manipulation statements. As such, programming languages must be expressive yet simple to use, unambiguous and preferably <a href="http://en.wikipedia.org/wiki/Formal_verification">verifiablee</a>. Expressiveness, simplicity and verifiability are a tough bunch to triage.</p>

<h2>Formal Techniques</h2>

<p>Systems such as <a href="http://en.wikipedia.org/wiki/Hoare_logic">Hoare logic</a>, <a href="http://en.wikipedia.org/wiki/Specification_language">algebraic specification languages</a> and <a href="http://en.wikipedia.org/wiki/Denotational_semantics">denotational semantics</a> are powerful formal verification methods but demand a great deal of sophistication on the part of the programmer and are often impractical as a result. <a href="http://en.wikipedia.org/wiki/Type_system">Type systems</a> encompass formal methods which are sufficiently tractable to be widely applicable, yet mainstream programming languages usually support only the tip of the iceberg of the theoretical capabilities. For instance, <a href="http://en.wikipedia.org/wiki/Algebraic_data_type">algebraic data types</a> such as <a href="http://msdn.microsoft.com/en-us/library/dd233226.aspx">discriminated unions</a> and the associated <a href="http://en.wikipedia.org/wiki/Pattern_matching">pattern matching</a> techniques are powerful mechanisms for expressing domain knowledge. Yet these techniques aren't available in mainstream OOP languages such as Java, C#, etc. They are available in functional languages such as F#, but even most functional languages don't support higher order techniques such as the polymorphic lambda calculus <a href="http://en.wikipedia.org/wiki/System_F">System F</a>. This seemingly relentless friction leads to some concerning questions. Are modern programming languages approaching the boundaries of the balance between power and accessibility? Will programmers need to embrace more advanced formal techniques in order to advance the state of the art?</p>

<h2>Knowledge-Driven Architectures</h2>

<p>All of the above-mentioned paradigms share a common goal of facilitating the conversation between humans and computers. <a href="http://en.wikipedia.org/wiki/Semantic_architecture">Semantic architectures</a> embody yet another approach to distilling knowledge in software systems. Semantic architectures involve technologies and practices such as <a href="http://en.wikipedia.org/wiki/Ontology_engineering">ontological engineering</a>, the <a href="http://en.wikipedia.org/wiki/Semantic_Web">semantic web</a> and the <a href="http://en.wikipedia.org/wiki/Web_Ontology_Language">Web Ontology Language (OWL)</a>. These relatively new fields of computer science evolved from the observation that domain knowledge is the the most important aspect of a computer system. In order to be practical, <a href="http://groups.csail.mit.edu/medg/ftp/psz/k-rep.html">knowledge representation</a> schemes should allow not only for expressive but also for seamless integration with the infrastructure. Ontology languages such as <a href="http://en.wikipedia.org/wiki/CycL">CycL</a> aim to provide such environments. With <a href="http://semanticweb.com/it-of-the-future-semantic-cloud-architecture_b31649">IT of the Future: Semantic Cloud Architecture</a> <a href="http://www.linkedin.com/pub/jeff-yefim-zhuk/6/76b/3a">Jeff Zhuk</a> outlines a transition from existing SOA architectures to novel knowledge-driven architectures. Knowledge-driven architectures aim to align business and IT and eliminate duplication of knowledge. In this way, they are an evolution of the SOA vision.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Abstractions]]></title>
    <link href="http://gorodinski.com/blog/2012/05/31/abstractions/"/>
    <updated>2012-05-31T00:02:00-04:00</updated>
    <id>http://gorodinski.com/blog/2012/05/31/abstractions</id>
    <content type="html"><![CDATA[<p>Abstraction is a fundamental operation of the mind. (According to <a href="http://en.wikipedia.org/wiki/Abstraction#Origins">Wikipedia</a>, abstraction is believed to have developed between 50,000 and 100,000 years ago). In philosophy, <a href="http://en.wikipedia.org/wiki/John_Locke">Locke</a> views abstraction as the act of separating from ideas all other ideas which accompany them in their real existence. In computer science, <a href="http://en.wikipedia.org/wiki/Edsger_Dijkstra">Dijkstra</a> views abstraction as the creation of new semantic levels. Locke's definition is structural in that abstraction tends to remove aspects of an idea, especially aspects that make an idea concrete. Dijkstra's definition emphasizes the goal of abstraction. After all, it doesn't seem sufficient for abstraction to be the separation of <strong>arbitrary</strong> aspects from ideas. Instead, it is the chiseling away of <strong>specific</strong> characteristics that bring value to abstraction. Characteristics are removed yielding an idea that isolates interesting components of a less abstract idea. One advantage of this process is that of generality - propositions believed to be true about an abstract notion should also hold for the more specific notion. Another advantage is that of encapsulation - references can be made to abstractions instead of concretions thereby simplifying the problem space by disregarding unimportant details. <a href="http://www.joelonsoftware.com/">Joel Spolsky</a>, with the <a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">Law of Leaky Abstractions</a>, observes that "All non-trivial abstractions, to some degree, are leaky". This observation alludes to the dark side of abstraction - the side where abstraction is misguided, shortsighted and of course leaky. The law stems from the software engineering perspective, however, it is applicable to all manifestations of abstraction. In software engineering in particular, the act of abstraction is pervasive - function declarations, class declarations, assignment of variables are all acts of abstraction. As such, software engineering also provides ample opportunity for leaky and needless abstractions.</p>

<!--more-->


<p>In hindsight, abstraction is absolutely fascinating. Without it, one would be hard-pressed to envision human thought let alone language, mathematics, art, etc. Abstraction breeds generalization and these concepts are perhaps shadows of a single principle cast in different directions. Abstraction itself hinges upon another capacity of thought, <a href="http://en.wikipedia.org/wiki/Symbol">symbology</a>, which at the most fundamental level is association between entities. And thus encapsulation is made - with association between a symbol and its meaning. Encapsulation is a mechanism for managing complexity in the face of limits upon resources of the mind. The mind can only have a certain number of things under consideration and without abstraction, generalization and encapsulation, compound ideas could not be formed. We've now derived Locke's three acts of the mind:</p>

<p><blockquote><p>The acts of the mind, wherein it exerts its power over simple ideas, are chiefly these three: 1. Combining several simple ideas into one compound one, and thus all complex ideas are made. 2. The second is bringing two ideas, whether simple or complex, together, and setting them by one another so as to take a view of them at once, without uniting them into one, by which it gets all its ideas of relations. 3. The third is separating them from all other ideas that accompany them in their real existence: this is called abstraction, and thus all its general ideas are made.</p><footer><strong>John Locke</strong> <cite>An Essay Concerning Human Understanding (1690)</cite></footer></blockquote></p>

<p>The concept of abstraction can be illustrated in a variety of ways, all <a href="http://bit.ly/dbeIfU">reducible</a> to each other. In a sense, the different portrayals of abstraction are different manifestations and implications of a fundamental axiom. Abstraction can be viewed as representation in that an abstract entity <em>represents</em> a concrete entity. An abstraction can also be defined as a <a href="http://en.wikipedia.org/wiki/Subset">strict subsets</a> of commonalities between things.</p>

<h2>Linguistics</h2>

<p>In linguistics, abstraction emerges in various forms and one particularly peculiar form is the abstraction hierarchy that is the relation among <a href="http://en.wikipedia.org/wiki/Syntactics">syntax</a>, <a href="http://en.wikipedia.org/wiki/Semantics">semantics</a>, and <a href="http://en.wikipedia.org/wiki/Pragmatics">pragmatics</a>. Syntactics, is the study of relations of signs to one another. This is an abstraction of relations between signs and meanings, which is semantics. Semantics, in turn, is an abstraction of pragmatics which is the study of relations between signs and contextual interpretations. This hierarchical aspect of abstraction is also revealed in linguistics as relations between words ordered by generality, such as for instance the ordering between <a href="http://en.wikipedia.org/wiki/Noam_Chomsky">Noam Chomsky</a>, a specific person and the simply <a href="http://en.wikipedia.org/wiki/Person">person</a>, a more general term that encapsulates references to any person.</p>

<h2>Nature</h2>

<p>Abstraction is not a human made creation but is present throughout nature. Abstraction at the <a href="http://en.wikipedia.org/wiki/Quantum_mechanics">quantum level</a> allows for the formation of compound structures such as atoms and molecules which in turn combine into cells and ultimately into all forms of life. It is no surprise then that abstraction manifests in human <a href="http://en.wikipedia.org/wiki/Cognition">cognition</a> which can be viewed as an extension of the complex chain of abstractions leading up to it, as an <a href="http://www.iscid.org/encyclopedia/Emergent_Properties_of_Biological_Systems">emergent behavior</a>. The <a href="http://en.wikipedia.org/wiki/Brain">brain</a> is the <a href="http://www.wired.com/science/discoveries/news/2008/01/connectomics?currentPage=all">most complex structure in the universe</a> and perhaps computer systems will have to emulate biology beyond <a href="http://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a> in order to exhibit such complexity.</p>

<h2>Mathematics</h2>

<p><a href="http://en.wikipedia.org/wiki/Category_theory">Category theory</a> is a pinnacle of abstraction in mathematics. It purports to formally unify all mathematical disciplines by way of abstractions - collections of <em>objects</em> and <em>arrows</em>. An example from an older discipline of Group Theory, follows. The <a href="http://en.wikipedia.org/wiki/Abelian_group">Abelian group</a> is an abstraction of the familiar integers (more specifically the operation of addition of integers). The group is named after, <a href="http://en.wikipedia.org/wiki/Niels_Henrik_Abel">Niels Henrik Abel</a>, who independently invented group theory to prove a theorem regarding solutions to 5th degree polynomials. This is an example of the power of generality in mathematics - it makes reasoning about certain aspects of mathematics more natural and even elegant.</p>

<h3>Mathematical Physics</h3>

<p>In 1928, With the <a href="http://en.wikipedia.org/wiki/Dirac_equation">Dirac equation</a>, Paul Dirac postulated the existence of the <a href="http://en.wikipedia.org/wiki/Positron">positron</a>. The mathematical model allowed for an electron with positive charge which seemed to contradict experimental results at the time. Ultimately, Dirac's equation lead to a remarkable discovery in quantum physics and an eerie and intimate relationship between mathematics and reality. This marks yet another mathematical abstraction success story where mathematics, a discipline devised by humans, makes predictions about the physical world. In comparison to other disciplines, mathematics has an advantage in that it is self-defined and purified from real world concerns. Definitions and problem statements are reduced to elementary concepts which allows abstraction to thrive. By contrast, abstractions in programming can become "leaky" due to forces that cannot be removed from consideration.</p>

<h2>Computer Science / Software Engineering</h2>

<p>Abstraction is a cornerstone of computer programming and it echoes throughout the hierarchy starting with machine language, continuing with the C programming language and leading all the way up to user facing application components such as windows, buttons, etc. Low-level programming languages, such as assembly, are termed as such due to the low degree of abstraction between the language and the underlying hardware. Low-level languages are elementary, but difficult to use for construction of complex applications because of the impedance mis-match between human thought and machine codes. Furthermore, a low degree of abstraction implies a high degree of coupling to the underlying hardware thereby reducing portability. High-level programming languages consist of statements and expressions that are closer to natural language. Since high-level programming languages must still run on a computer they are translated to lower level languages, ultimately resulting in machine code. This translation process creates a layer of abstraction, which enables increased portability and expressiveness. The need for translation however is also a cause of trouble for abstractions in computer programming. The reality is that the mechanics of the underlying hardware cannot be entirely escaped and are bound to creep into higher levels if not managed properly. Take for example <a href="http://bit.ly/qaOHl">garbage collection</a> which purports to abstract away memory management. While a powerful abstraction it carries a set of compromises that cannot be overlooked. One is determinism - since required memory manipulation operations are executed behind the scenes, the programmer cannot be certain of the memory state of the system. Another compromise is hidden complexity - while for the most part programmers can forget about memory management, they must also be aware of a complex GC system at play to understand certain intricacies of their programs.</p>

<h3>OOP</h3>

<p>In <a href="http://en.wikipedia.org/wiki/Object-oriented_programming">OOP</a>, classes and interfaces are mechanisms of abstraction. The concept of inheritance in OOP borrows from inheritance in nature and allows specialization and reuse. Classes, however, support only a narrow view of abstraction. Any graduate of an entry level OOP course should be able to understand the difference between a class and an instance of a class - an object. The difference however is very much arbitrary. An instance of a class stores state in fields declared in the class definition. In this way, an instance of a class is distinguished from the class itself - it can store specific values. What if the instance could not only hold state, but can be augmented with new behaviours? Is it still the same "class", or is it a new thing all together? From the perspective of syntax the difference between a class and an object is evident. From a perspective outside of syntax, an object can be viewed as a class for a whole new set of objects which derive from and specialize the class in some way. <a href="http://en.wikipedia.org/wiki/Prototype-based_programming">Prototype-based programming</a> languages, such as JavaScript, don't utilize classes as abstraction mechanisms, instead supporting the cloning of objects allowing any object to serve as a prototype for another. This is a more fluid and flexible approach to abstraction, although compromising on some of the benefits of OOP.</p>

<h3>MDA</h3>

<p><a href="http://en.wikipedia.org/wiki/Model-driven_architecture">Model-Driven architecture</a> aims to raise the level of abstractions in software engineering beyond specific development platforms with use of <a href="http://en.wikipedia.org/wiki/Domain-specific_language">domain-specific languages</a> and transformation tools. However, as of today, MDA techniques have yet to gain industry acceptance or demonstrate a concrete value proposition. This is due to a variety of <a href="http://en.wikipedia.org/wiki/Model-driven_architecture#MDA_concerns">reasons</a> and perhaps there is a limit to the degree of abstraction attainable, at least with current approaches.</p>

<h3>Von Neumann Architecture</h3>

<p>As another testament to elevating abstractions, <a href="http://en.wikipedia.org/wiki/John_Backus">John Backus</a> called for the liberation of programming from the <a href="http://en.wikipedia.org/wiki/Von_Neumann_programming_languages">Von Neumann style</a> in his Turing Award <a href="http://www.cs.cmu.edu/~crary/819-f09/Backus78.pdf">lecture</a>. The vast majority of modern computers are based on the Von Neumann Architecture and the vast majority of programming languages are abstract <a href="http://en.wikipedia.org/wiki/Isomorphism">isomorphisms</a> of this architecture. This basically means that there is a one-one mapping between the hardware and software. Backus coined the term "Von Neumann bottleneck" in reference to both hardware and software limitations that are byproducts of this isomorphism. In the hardware context there is a literal bottleneck which inhibits performance due to limits to data transfer between <a href="http://en.wikipedia.org/wiki/Computer_memory">memory</a> and the <a href="http://en.wikipedia.org/wiki/Central_processing_unit">processing unit</a>. In the intellectual sense, there is a bottleneck which inhibits reasoning about programs because the architecture encourages a variable-at-a-time thinking. Backus proposes <a href="http://en.wikipedia.org/wiki/Functional_programming">functional programming languages</a> as an evolution of the Von Neumann style and over 30 years after his lecture, his propositions are becoming mainstream. Unfortunately, we've yet to witness significant advancements beyond the Von Neumann bottleneck and it remains one of the challenges of abstraction in computer science.</p>

<h3>Success</h3>

<p>Although great challenges lie ahead, the fields of software and hardware engineering have had immense success with abstractions. Just consider all the intricacies involved in something as simple as opening a website as elucidated in <a href="https://plus.google.com/112218872649456413744/posts/dfydM2Cnepe">Dizzying but invisible depth</a>. It is a humbling experience to even begin to fathom the countless moving parts, the years of the evolution of human knowledge, the countless brilliant minds that make this seemingly simple action possible.</p>

<h3>Needless Abstractions</h3>

<p>Needless abstractions are abstractions that don't bring value. The definition of value, of course, is subjective which highlights the qualitative nature of abstraction. In other words, not all abstractions are created equal - if abstraction is to create new semantic levels and manage complexity it must be invoked in ways appropriate to a given context. Needless abstractions arise when the programmer's mental model of the program is reflected directly in code without regard for its utility. An interface may be extracted from a class thereby creating an abstraction, however if the interface isn't used then it brings no value. Instead it can raise confusion and unnecessary dependencies. Avoiding needless abstractions can be difficult because designing abstractions is what programming is all about. The programmer scans his or hers mental model, detecting commonalities, forming composites, mapping relationships and this process can create abstraction-waste-by-product. An example from enterprise development is the <a href="http://codebetter.com/gregyoung/2009/01/16/ddd-the-generic-repository/">generic repository</a>. While the intent is alluring - to reuse and generalize data access code, the drawbacks outweigh the benefits. Moreover, the apparent benefits are misguided and can be attained using more appropriate methods.</p>

<p>The drive for abstractions is a natural consequence of programming, however it must be kept in balance, as often presented by <a href="http://ayende.com/blog/">Ayende</a> with his "Limit your Abstractions" series. Conversely, avoiding abstractions for fear of complexity shouldn't be the default position, because well designed abstractions have the potential to improve clarity, reuse and ultimately advance the state of the art. Additionally, abstractions can furnish encapsulation, which is much needed in programming where one must be capable of switching between levels of abstraction different by orders of magnitude.</p>

<p>When designing abstractions, it is beneficial to keep in mind principles such as <a href="http://www.codinghorror.com/blog/2004/10/kiss-and-yagni.html">YAGNI and KISS</a>. <em>True wisdom is held in combating complexity with expressiveness, not at the cost of expressiveness - just enough abstraction, but no more.</em> Needless abstractions can emerge at all scopes of software engineering, from the developer to the architect. After all, developers and architects are on different ends of the same spectrum the central distinction being that architects analyze applications at higher levels of abstraction. Many of the driving forces however are the same, which is why many times patterns that apply at the class level are also applicable at the system integration level.</p>

<h3>Abstraction Challenges</h3>

<p>The leap in abstraction between machine language and C, achieved in the <a href="http://bit.ly/KPfUX">early seventies</a>, is yet to be surpassed. For instance, the leap in abstraction between C and C# is minimal and owes more to <a href="http://bit.ly/qaOHl">garbage collection</a>, runtime libraries and <a href="http://en.wikipedia.org/wiki/Syntactic_sugar">syntactic sugar</a> than to semantics and expressiveness. Both languages are <a href="http://en.wikipedia.org/wiki/Third-generation_programming_language">third generation</a>. Fourth-generation languages are domain-specific pockets of higher levels of abstraction and declarative-ness, such as SQL. Fifth-generation languages, such as <a href="http://en.wikipedia.org/wiki/Prolog">Prolog</a>, are further abstractions away from machine language however we're still far from working with levels of abstraction accessible to humans. Japan's failed <a href="http://en.wikipedia.org/wiki/Fifth_generation_computer">5th generation computer</a> project is a paragon of the challenges of abstraction. There seems to be a bottleneck in our current approaches and methodologies. If we encounter challenges with something seemingly simply as <a href="http://en.wikipedia.org/wiki/Object-relational_mapping">object-to-relational mapping</a> then surely we will encounter challenges in mapping program code to natural thought. How do we map Von Neumann machines to a largely uncharted network of billions of neurons? Is there a limit to what can be achieved with current hardware technology? In order to attain greater sophistication, will computers have to become more like biological organisms? Will this compromise determinism?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Is the universe simple or complex?]]></title>
    <link href="http://gorodinski.com/blog/2008/07/29/is-universe-simple-or-complex/"/>
    <updated>2008-07-29T17:10:00-04:00</updated>
    <id>http://gorodinski.com/blog/2008/07/29/is-universe-simple-or-complex</id>
    <content type="html"><![CDATA[<p>The <a href="http://en.wikipedia.org/wiki/Law_of_large_numbers">Law of Large Numbers</a> and the <a href="http://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> imply that the world is in a state that has the highest probability in relation other conceivable states. The previous statement refers to the world as a whole and thus minor variations in the probabilities of proximal states are expected. The fundamental technique in the mentioned mathematical theories is that of measuring the likelihood of an outcome based on a determined function. In particular cases, such as the familiar deck of cards, this function is derived from ratios of expected cards versus the entire sample space - the deck. It is reasonable to believe that the things that happen, in a large environment and over a sufficient period of time, are the things that have the highest probability. In fact this empirical data is where the probability functions derive from. The question here is whether these things that do happen are simple or complex. And surely the next question asks for the definition of simplicity and complexity.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The inverse syllogism within evolution and AI]]></title>
    <link href="http://gorodinski.com/blog/2008/07/29/inverse-syllogism-within-evolution-and-ai/"/>
    <updated>2008-07-29T17:10:00-04:00</updated>
    <id>http://gorodinski.com/blog/2008/07/29/inverse-syllogism-within-evolution-and-ai</id>
    <content type="html"><![CDATA[<p>A.I. purports to understand thought and create systems that are able to perform thought, or think. In this endeavor we attempt to observe, study and analyze our own thinking, and make conclusions about thinking in general. These conclusions become the axioms in the syllogisms of Aristotle. Subsequently, traditional intelligence models follow this paradigm. An important observation is that these models are not only designed to follow syllogisms, but are also arrived at in the same fashion. This however is in great contrast to how our own intelligence evolved. Our own intelligence evolved through millions of years of random trial and error, extinction and survival. The logical backbone behind the rationality humans posses is the confluence of our environment and the challenges it entails for survival, which of course is a requisite for any sort of intelligence to exist.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Frontiers of thought]]></title>
    <link href="http://gorodinski.com/blog/2008/04/05/frontiers-of-thought/"/>
    <updated>2008-04-05T10:17:00-04:00</updated>
    <id>http://gorodinski.com/blog/2008/04/05/frontiers-of-thought</id>
    <content type="html"><![CDATA[<p>An interesting challenge arises in abstract matematics when it is required to translate intuitive notions into formalisms. The existence of a particular formalism is always uncertain and the path to its discovery is often a walk on thin ice challenging mathematics at each step.</p>
]]></content>
  </entry>
  
</feed>
