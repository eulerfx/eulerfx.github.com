<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: NHibernate | Lev Gorodinski]]></title>
  <link href="http://gorodinski.com/blog/categories/nhibernate/atom.xml" rel="self"/>
  <link href="http://gorodinski.com/"/>
  <updated>2014-06-12T11:59:18-04:00</updated>
  <id>http://gorodinski.com/</id>
  <author>
    <name><![CDATA[Lev Gorodinski]]></name>
    <email><![CDATA[eulerfx@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Porting from SQL with NHibernate to RavenDB]]></title>
    <link href="http://gorodinski.com/blog/2012/08/28/porting-from-sql-with-nhibernate-to-ravendb/"/>
    <updated>2012-08-28T19:45:00-04:00</updated>
    <id>http://gorodinski.com/blog/2012/08/28/porting-from-sql-with-nhibernate-to-ravendb</id>
    <content type="html"><![CDATA[<p><em>In this post I will overview a port of a persistence implementation of a Domain-Driven Design project from a solution based on SQL Server and <a href="http://nhforge.org/Default.aspx">NHibernate</a> to a solution based on <a href="http://ravendb.net/">RavenDB</a>. I describe the problem domain, the current stack, project goals, the database selection process, as well as serialization, indexing and optimization patterns.</em></p>

<!--more-->


<h2>Domain</h2>

<p>The domain is product pricing of an online retailer and accordingly the primary functionality is the calculation of prices for products based on multiple variables. The central aggregate in this domain is a product pricing strategy which consists of price calculations for each supplier of the product and is keyed by the <a href="http://martinfowler.com/eaaCatalog/identityField.html">ID</a> of the product. Prices are updated for a variety of reasons and the trigger can be event based or manual. The events which trigger a price update include any events that may result in a price change, such as a supplier cost change. Manually triggered price updates are invoked by members of the ecommerce team and are happen for a variety of reasons, such as seasonal experimentation. Manual updates are initiated by a selection query which defines the set of products to update. This selection query filters by attributes belonging to products, such as product category, as well as attributes belonging to the product's pricing strategy, such as margin. Products themselves are <a href="http://devlicio.us/blogs/casey/archive/2009/02/16/ddd-aggregates-and-aggregate-roots.aspx">aggregates</a> from a different <a href="http://domaindrivendesign.org/node/91">bounded context</a> and can be entities or value objects in the pricing context. Currently, the system manages pricing for approximately 2 million products.</p>

<h2>Stack</h2>

<p>The system runs on the .NET platform, uses SQL Server for persistence and NHibernate for the <a href="http://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a>. Messaging is implemented with <a href="http://nservicebus.com/">NServiceBus</a>. Business logic is implemented using Domain-Driven Design. The functionality is exposed via HTML interface delivered by <a href="http://www.asp.net/mvc">ASP.NET MVC</a> and <a href="http://www.asp.net/web-api">ASP.NET WebAPI</a>.</p>

<h2>Goals</h2>

<p>The port project had several goals, including a streamlined mapping with the persistence layer, more advanced querying capabilities and moving as much logic out of the database as possible. A performance improvement was to be a bonus. Additionally, severing the dependence on SQL Server was certainly a plus. Given these constraints and the fact that the existing system was implemented using Domain-Driven Design, a NoSQL document database was deemed an appropriate solution. The document database had to be able to both store and query serialized representations of domain entities. Additionally, some of the more elaborate querying requirements would call for <a href="http://en.wikipedia.org/wiki/MapReduce">Map/Reduce</a> capabilities.</p>

<h2>NoSQL database selection</h2>

<p>There are numerous document databases available in the wild that can satisfy the goals of the project. Among the ones considered were RavenDB, MongoDB, CouchDB, SimpleDB, DynamoDB, Azure Table Storage, Cassandra, HBase and Redis. SimpleDB does provide decent querying capabilities, but tables are limited to 10GB and performance seemed questionable. DynamoDB, while highly performant, is a key-value store which means that it doesn't support secondary indexes. Azure Table Storage has similar drawbacks. Cassandra and HBase can both fulfill all stated requirements, however the configuration and calibration of all moving parts seemed like a hassle. Overall, they are more focused on performance and immense scalability than ease of use. Redis is better suited for storing performance sensitive soft real-time data and it is also a key-value store. Map/Reduce is possible with any NoSQL database, but it requires additional configuration. A good overview of the various NoSQL databases is provided by <a href="http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis">Kristof Kovacs</a>.</p>

<p>It came down to a decision between RavenDB, MongoDB and CouchDB. All three store serialized representations of entities, support complex queries as well as Map/Reduce. MongoDB has amassed an <a href="http://www.mongodb.org/display/DOCS/Production+Deployments">impressive client list</a> and its indexes are easy to grasp for SQL developers. In CouchDB, all querying is implemented with views which are generated via Map/Reduce. In RavenDB, queries are served by Lucene and the corresponding indexes are defined with <a href="http://msdn.microsoft.com/en-us/library/bb397926.aspx">LINQ</a> which also supports Map/Reduce declarations. After a bit of exploration, I discovered that a significant advantage that RavenDB had over the others was that its Map/Reduce indexes were updated automatically based on changes in related documents. MongoDB would require an out of band scheduled process to update Map/Reduce collections and CouchDB, by default, only updates Map/Reduce views on demand, not immediately after a document update. While MongoDB indexes are updated in place, some of the more intricate querying requirements would require use of Map/Reduce, as stated. Another advantage RavenDB had over the others is its support of transactions across multiple documents. This is an important feature because many use cases, in addition to storing and updating an entity, store a domain event.</p>

<p>During data migration, MongoDB exhibited better write performance - almost an order of magnitude faster than RavenDB. In testing actual use cases however, RavenDB was on par with MongoDB. Existing use cases involved loading a single aggregate, performing an operation and persisting the resulting changes and domain events. Since migration would only happen once, the advantage MongoDB has in terms of write performance was negligible overall.</p>

<p>Additionally, RavenDB runs on the .NET platform and is open-source. I've taken advantage of that on several occasions either by looking at the code to understand certain behavior, or writing <a href="http://stackoverflow.com/a/7731378/13855">extensions to existing functionality</a>. Ultimately, RavenDB was selected as the NoSQL document database for this project.</p>

<h2>Anatomy of RavenDB</h2>

<p>RavenDB is built on the .NET platform and makes use of two major components, the <a href="http://managedesent.codeplex.com/">Esent database engine</a> for document storage and <a href="http://lucene.apache.org/core/">Lucene</a> for indexes. Document storage is <a href="http://en.wikipedia.org/wiki/ACID">ACID</a> while Lucene indexes are updated in the background in an eventually consistent manner. This is effectively <a href="http://martinfowler.com/bliki/CQRS.html">CQRS</a> right out of the box! The <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> implies a continuum between ACID and BASE and for this particular project RavenDB resides in a sweet spot between the two. For serialization, RavenDB uses <a href="http://json.org/">JSON</a> via the <a href="http://james.newtonking.com/projects/json-net.aspx">Json.NET</a> library. The <a href="http://martinfowler.com/eaaCatalog/unitOfWork.html">Unit of Work</a> pattern is supported in a way similar to NHibernate with the <a href="https://github.com/ravendb/ravendb/blob/master/Raven.Client.Lightweight/IDocumentSession.cs">IDocumentSession</a> interface which is yet another advantage RavenDB has over the competition. RavenDB supports transactions as a durable resource manager from <a href="http://msdn.microsoft.com/en-us/library/system.transactions.aspx">System.Transactions</a>. Aside from enabling transactional semantics between multiple document sessions, this is helpful for database integration testing by allowing rollback of changes resulting from a test. The NHibernate solution already contained such integration tests which ended up being completely portable. One caveat is that <a href="http://ravendb.net/docs/faq/working-with-dtc">changes aren't visible until the transaction commits</a>, which is slightly different from the behavior of SQL Server.</p>

<h2>Serialization</h2>

<p>RavenDB automatically serializes objects upon persistence and deserializes them upon reconstitution. Serialization is performed automatically but can be customized in several ways. The simplest way is to use the <a href="http://msdn.microsoft.com/en-us/library/system.runtime.serialization.datamemberattribute.aspx">DataMember</a> attribute from the <a href="http://msdn.microsoft.com/en-us/library/system.runtime.serialization.aspx">System.Runtime.Serialization</a> namespace. For deeper control, serialization attributes from the Json.NET library can be used.</p>

<p>Beyond that, it is always possible to use an intermediate object which can serve as a <a href="http://martinfowler.com/eaaCatalog/dataTransferObject.html">DTO</a> between the object model and the database. In this way, the object model is completely decoupled from the persistence layer. However, the intermediate object approach has several drawbacks. One drawback is that it adds a layer of indirection which is prone to error and requires maintenance. Another drawback is that it makes use of the unit of work pattern more difficult, because RavenDB will track changes to the persisted DTO object, not the corresponding domain object.</p>

<p>The best approach to serialization is by convention which can be done by customizing the <strong>JsonSerializer</strong> and the <strong>JsonContractResolver</strong> used by RavenDB as described <a href="http://ravendb.net/docs/client-api/advanced/custom-serialization">here</a>. The default conventions are mostly satisfactory except that public properties without a setter are also serialized, which of course will throw an exception upon deserialization. It should also be noted that currently, <a href="https://github.com/ayende/ravendb/blob/2df28c143330649569f223aeb3b87b4c9f2ad5ea/Raven.Client.Lightweight/Document/SessionOperations/QueryOperation.cs">RavenDB creates</a> a new instance of the <strong>JsonSerializer</strong> when deserializing each individual document. This means that if a customized <strong>JsonContractResolver</strong> is provided, cache sharing must be enabled so that performance doesn't suffer.</p>

<p>Another applicable axis of customization is overriding the type names stored for polymorphic properties in the <strong>$type</strong> field. By default, Json.NET will generate a type discriminator value which is the namespace and assembly qualified name of the type. While this gets the job done, a cleaner, more portable and compact approach is to store a context specific discriminator value. This can be done by overriding the default implementation of <a href="http://msdn.microsoft.com/en-us/library/system.runtime.serialization.serializationbinder.aspx">SerializationBinder</a> as described <a href="http://stackoverflow.com/a/12203624/13855">here</a>.</p>

<h3>Refactoring</h3>

<p>Serialization is also important in the context of refactoring. If domain entities are persisted directly then changing the name of a member, or the name or namespace of a type will break serialization. Changes to member names can be facilitated by maintaining a serialization attribute with a hard-coded field name or by using the <a href="http://ravendb.net/docs/client-api/partial-document-updates">Patching API</a> to propagate the changes to the entire document set. The former breaks persistence ignorance while the latter can require a lot of processing time. Despite the issues, a document database facilitates a predominant degree of <a href="http://gorodinski.com/blog/2012/08/10/toward-persistence-ignorance-with-nhibernate-in-domain-driven-design/">persistence ignorance</a> as compared to their relational database counterparts.</p>

<h2>Indexing</h2>

<p>RavenDB indexes are defined through the <a href="https://github.com/ravendb/ravendb/blob/master/Raven.Abstractions/Indexing/IndexDefinition.cs">IndexDefinition</a> class. This class allows the declaration of one or more mapping functions and an optional reduce function with LINQ. It also allows the specification of a post query result transformation function as well as field sorting and indexing options. The map functions select properties from existing documents for indexing and only selected properties will be indexed and made queryable. The optional reduce function can be used to perform aggregation upon properties selected by the map functions providing functionality roughly equivalent to the <a href="http://msdn.microsoft.com/en-us/library/ms177673.aspx">group by</a> clause.</p>

<p>Indexes can also be defined in a more strongly typed fashion by inheriting from <a href="https://github.com/ayende/ravendb/blob/master/Raven.Client.Lightweight/Indexes/AbstractIndexCreationTask.cs">AbstractIndexCreationTask</a> as seen <a href="http://ravendb.net/docs/client-api/querying/static-indexes/defining-static-index">here</a>. In this way, the map, reduce and transform functions are declared directly in the IDE instead of a raw string. For this project however, the strongly typed approach fell short because there was a requirement to index properties of polymorphic types as described <a href="http://stackoverflow.com/questions/11942486/index-type-discriminator-and-properties-of-polymorphic-type-in-ravendb">here</a>. As a result, I had to resort to the weakly typed <strong>IndexDefinition</strong> route.</p>

<p>The <a href="http://ayende.com/blog/89089/ravendb-multi-maps-reduce-indexes">multi-map/reduce</a> indexing functionality allows selection of properties from documents in multiple collections. This came in handy for this particular project because, as stated, the selection query needs to be able to filter by attributes of both products and pricing strategies. One way to achieve this is by storing the product and pricing strategy in a single document. In this way, the product is a value object of the corresponding pricing strategy, which is not problematic since products are already sourced from a different bounded context. However, given that products have a different life-cycle than the associated pricing strategies and for purely exploratory purposes, a similar outcome was achieved by using a multi-map index. This type of index contains multiple map functions which can select documents from multiple collections, products and pricing strategies in particular. The results are effectively "joined" in the reduce function. The declaration of a multi-map in this type of scenario can be a bit awkward. All of the map functions as well as the reduce function have to produce output having an identical shape. The product and pricing strategy documents contain entirely distinct properties and thus the corresponding map functions had to declare properties that not part of the source document as nulls. Moreover, the reduce function, after joining the documents on a common key which was the product ID, had to select the not-null properties from the group for the final output. Perhaps a better API for this particular scenario would be something similar to the <a href="http://ravendb.net/docs/client-api/querying/static-indexes/live-projections">live projections API</a>:</p>

<p><div><script src='https://gist.github.com/3629525.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/3629525&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>
</p>

<p>By comparison, the live projections transform function is invoked at query time, not indexing time.</p>

<p>A caveat of RavenDB indexing is indexing speed. Indexes are updated in the background and the system remains responsive during the entire process, but generating the index for the selection query takes several hours and I've even experienced it taking over a day on occasion. I found that it is more performant to load all of the data before creating the indexes. This can affect the development, exploration and iteration workflow, because changes to indexes cannot be taken for granted. Generally, this is symptomatic of the shift from relational databases to document databases - no more ad hoc.</p>

<h2>Optimization</h2>

<p>RavenDB performs well out of the box but also provides several options for tweaking. For this project like many others, the biggest gain in performance was achieved with batching. As described, one of the use cases involved a selection query which was used to select the products for a price update. The query may select very large sets of products and strategies which would be queued for processing with NServiceBus. NServiceBus allows the batching of several messages into a single transport message. The transport message, which may contain multiple domain messages, is dequeued in a single unit of work and the individual domain messages are dispatched to the appropriate handlers. <a href="http://andreasohlund.net/">Andreas Ohlund</a> described a <a href="http://andreasohlund.net/2011/11/22/a-nservicebus-unit-of-work-implementation-for-ravendb/">unit of work adapter</a> between NServiceBus and RavenDB which allows RavenDB to batch multiple update requests.</p>

<p><a href="http://ayende.com/blog/4584/ravendb-includes">RavenDB includes</a> were used in this project to batch requests to retrieve a product and the corresponding pricing strategy. Includes instruct RavenDB to retrieve a document together with an associated document which will be accessible through the same <strong>IDocumentSession</strong>. This is a great optimization because it happens behind the scenes and calling code doesn't have to change.</p>

<p>Another performance gain was achieved by altering the transaction mode with the <em>Raven/TransactionMode</em> setting. Setting the mode to "Lazy" is faster but can result in data loss in case of a server crash. The <a href="http://ravendb.net/docs/server/administration/configuration">RavenDB settings documentation</a> also recommends that indexes should be stored on a drive different from the data, though the performance benefit was negligible in this particular project.</p>

<h2>Conclusion</h2>

<p>All project goals were met, including a sizable performance gain. The mapping between the database and the code changes from a object-relational mapping problem to a serialization problem which is much easier to manage. Furthermore, given Domain-Driven Design, the mapping between aggregate and document is <a href="http://en.wikipedia.org/wiki/Bijective_function">one-to-one</a> which also addresses some of the transactional consistency concerns associated with document databases. The querying capabilities of RavenDB via Lucene are beyond the capabilities of a relational database. Lack of support for anything resembling stored procedures ensures that no business logic resides in the database - only data. Severing the dependence on SQL Server is beneficial for cost, resource and maintenance constraints, which are especially pronounced in a cloud environment. The overall performance gain was three-fold for this particular project, although the testing methods weren't meant to be precise.</p>

<p>The central "drawback" of document databases in general is the paradigm shift in terms of both data modeling as well as tooling and management. There is bigger pressure to design proper aggregate boundaries and understand querying requirements. This of course isn't a bad thing.</p>

<p>As a noteworthy piece of trivia, the entire RavenDB client API, including 3rd party libraries, can almost fit on an old-school floppy disk at 1.44 MB as of build 960. For comparison, the NHibernate v3.2 DLL weighs in at 3.9 MB. Together with all required libraries, an NHibernate stack can easily surpass the size of the RavenDB <em>server</em> DLLs which amount to 5.86 MB. Simplicity prevails!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Toward persistence ignorance with NHibernate in Domain-Driven Design (DDD)]]></title>
    <link href="http://gorodinski.com/blog/2012/08/10/toward-persistence-ignorance-with-nhibernate-in-domain-driven-design/"/>
    <updated>2012-08-10T12:35:00-04:00</updated>
    <id>http://gorodinski.com/blog/2012/08/10/toward-persistence-ignorance-with-nhibernate-in-domain-driven-design</id>
    <content type="html"><![CDATA[<p>Persistence ignorance is a quality of a design wherein entity classes are devoid of data access supporting characteristics. Adherence to this technique facilitates a separation between domain and infrastructure. By deflecting the technical noise, this separation streamlines the mapping between the model and its incarnation in code. In practice, persistence ignorance is an ideal seldom achieved in its entirety. Even if entities are stripped of all persistence related traits, the design tends to yield to influence from the persistence framework. Fortunately, like most things in programming, persistence ignorance falls upon a spectrum and can pay dividends even with partial application. After all, any type of code isn't an entirely noiseless medium.</p>

<!--more-->


<p>In domain-driven design, one way to attain persistence ignorance is with the use of a <a href="http://devlicio.us/blogs/casey/archive/2009/02/20/ddd-the-repository-pattern.aspx">repository</a> and an <a href="http://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a>. On the .NET Framework, the ORM <a href="http://en.wikipedia.org/wiki/NHibernate">NHibernate</a> affords a relatively high degree of persistence ignorance with the use of out of band mapping declarations and reflection. What follows is a summary of various techniques for achieving persistence ignorance with NHibernate.</p>

<h4>Default protected constructor</h4>

<p>NHibernate requires that mapped classes provide a default (parameterless) constructor. This allows NHibernate to instantiate the mapped class during reconstitution. Although the footprint is relatively minor, this is an impure and unavoidable constraint for mapped entities. NHibernate could potentially employ a constructor injection technique, as utilized by <a href="http://james.newtonking.com/projects/json-net.aspx">JSON.NET</a>. If the default constructor is marked as protected then non-reflective client code cannot directly instantiate the class thus still restricting access with other constructors. An <strong>ImplicitAttribute</strong> could be created to mark protected constructors to prevent productivity tools from flagging them as unused.</p>

<h4>Mapped member access strategies</h4>

<p>NHibernate is capable of mapping database fields to object properties which have private setters. This is one of the most basic methods for maintaining persistence ignorance. Entities and value objects encapsulate domain concepts and as such should regulate access to their internal data. Private setters ensure that class data is only modified by the class itself thereby limiting scope and facilitating encapsulation. With use of <a href="http://www.nhforge.org/doc/nh/en/#mapping-declaration-property">access and naming strategies</a>, NHibernate can map to a backing field by convention. This can be useful when having a property setter invoked is undesirable during reconstitution, such as when the setter implements domain logic. If using <a href="http://www.fluentnhibernate.org/">FluentNHibernate</a> for mapping, private fields having no corresponding property member can be referenced using the <a href="https://github.com/jagregory/fluent-nhibernate/wiki/Mapping-private-properties"><strong>Reveal.Property<TEntity></strong></a> method.</p>

<h4>Read-only collection mapping</h4>

<p>A common pattern for implementing an aggregation relationship in DDD is with a collection coupled with a method which manipulates it. For example:</p>

<p><div><script src='https://gist.github.com/3355179.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/3355179&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>
</p>

<p>The methods <strong>AddItem</strong> and <strong>RemoveItem</strong> encapsulate access to the collection of line items in the order entity. The collection can be effortlessly mapped as a <a href="http://www.nhforge.org/doc/nh/en/#collections-onetomany">one-to-many association</a>. A problem with this approach is that encapsulation can be easily broken because the collection property is publicly exposed and nothing restricts calling code from accessing it directly. To resolve this problem, a backing field access strategy can be used to map the collection to a private field. The property can then return a <a href="http://msdn.microsoft.com/en-us/library/ms132474.aspx">read-only wrapper</a> around the collection thereby isolating modification of the collection to class methods:</p>

<p><div><script src='https://gist.github.com/3355222.js?file='></script>
<noscript><pre><code>&lt;html&gt;&lt;body&gt;You are being &lt;a href=&quot;https://github.com/gist/3355222&quot;&gt;redirected&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;</code></pre></noscript></div>
</p>

<p>The methods now access the <strong>items</strong> field directly and attempts to modify the read-only collection returned by the <strong>Items</strong> property will throw a <a href="http://msdn.microsoft.com/en-us/library/system.notsupportedexception.aspx">NotSupportedException</a>.</p>

<h4>Lazy loading with virtual members</h4>

<p>By default, NHibernate requires class members to be virtual in order to support the <a href="http://en.wikipedia.org/wiki/Proxy_pattern">proxy pattern</a>; the proxy pattern in turn supports lazy loading. Lazy loading however <a href="http://gorodinski.com/blog/2012/06/16/orm-lazy-loading-pitfalls/">can be problematic</a> and the requirement to make all members virtual is certainly a infraction against persistence ignorance, despite how negligible. Fittingly, disabling lazy loading at the class mapping level waives this requirement.</p>

<h4>Custom mapping type</h4>

<p>To support mapping requirements that cannot be fulfilled by direct field to property mappings NHibernate provides the <a href="http://www.martinwilley.com/net/code/nhibernate/usertype.html"><strong>IUserType</strong></a> and <a href="http://geekswithblogs.net/opiesblog/archive/2006/08/05/87218.aspx"><strong>ICompositeUserType</strong></a> interfaces. They enable implementors to declare arbitrary mappings between a set of fields and a component type or value type. Arbitrarily complex value types can be mapped using this technique and since the implementor controls construction the mapped types don't need to provide a parameterless constructor.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ORM Lazy Loading Pitfalls]]></title>
    <link href="http://gorodinski.com/blog/2012/06/16/orm-lazy-loading-pitfalls/"/>
    <updated>2012-06-16T12:28:00-04:00</updated>
    <id>http://gorodinski.com/blog/2012/06/16/orm-lazy-loading-pitfalls</id>
    <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Object-relational_mapping">Object-relational mappers</a> furnish a mapping layer between <a href="http://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented code</a> and <a href="http://en.wikipedia.org/wiki/Relational_database">relational databases</a>. ORMs such as NHibernate and Entity Framework support lazy loaded associations which allow the loading of specific subsets of an object graph from an underlying relational store. This is beneficial because an object model can construct an object graph which is unfeasible to contain in main memory in its entirety. Lazy loading can prevent unnecessary data from being loaded and as such it is often presented as a performance optimization technique. This technique however incurs several drawbacks and is limited in its scalability. One drawback is that classes are static declarations and object associations will be accessible regardless of whether they are lazy loaded or eager loaded. As a result, it becomes more difficult to understand code because it isn't immediately certain whether navigating an association will result in a database call behind the scenes. Moreover, care must be taken to ensure that an ORM session is available lest we run into the dreaded <a href="http://docs.jboss.org/hibernate/core/3.5/api/org/hibernate/LazyInitializationException.html">LazyInitializationException</a>. Given that lazy loading is typically implemented using the <a href="http://en.wikipedia.org/wiki/Proxy_pattern">proxy pattern</a>, data access implementation details inevitably and invisibly leak into the rest of the application. In sense these characteristics can be regarded as a violation of the <a href="http://en.wikipedia.org/wiki/Principle_of_least_astonishment">principle of least astonishment</a>.</p>

<!--more-->


<p>The problem that lazy loading attempts to address can be illustrated with an analogy to the <a href="http://en.wikipedia.org/wiki/World_Wide_Web">world wide web</a>. The success of the word wide web can be attributed in part to its hyperlinked nature - resources are connected with links allowing for navigation of the web graph loading resources as they are needed. It is unrealistic for the entire web to be loaded into memory, <a href="http://jots.mypopescu.com/post/219463131/google-can-keep-all-web-in-memory">unless you are Google, of course</a>. A relational database can be viewed as a web which ORMs attempt to navigate while at the same time mapping relational data to an object model. The reality is that there is a subtle mismatch between the object model and the relational model. More accurately, No-SQL, corresponding to the object model, and SQL, corresponding to the relational model are <a href="http://bit.ly/KQoKA6">mathematical duals</a> as described by <a href="http://research.microsoft.com/en-us/um/people/emeijer/">Erik Meijer</a> in <a href="http://queue.acm.org/detail.cfm?id=1961297">"A co-Relational Model of Data for Large Shared Data Banks"</a>. <em>(Meijer was also responsible for the <a href="http://msdn.microsoft.com/en-us/data/gg577609.aspx">Reactive Extensions Framework</a> and demonstrating the duality between <a href="http://msdn.microsoft.com/en-us/library/9eekhta0.aspx">IEnumerable<T></a> and <a href="http://msdn.microsoft.com/en-us/library/dd990377.aspx">IObservable<T></a>).</em></p>

<p>In practice, the mismatch exists because SQL is best suited for ad hoc queries and ad hoc field selection whereas OOP is best suited for static models. From the relational perspective there is a tension to select data specifically for a given query which is in turn designed for a specific use case. From the OOP perspective there is a tension to conform the query to an object model which is designed for a variety of use cases. An important observation is that these mapping issues are most prominent on the query side of the equation. Consequently, a technique such as <a href="http://gorodinski.com/blog/2012/04/25/read-models-as-a-tactical-pattern-in-domain-driven-design-ddd/">read-models</a> can be utilized to mitigate lazy loading issues all together. Instead of devising intricate fetching strategies with lazy loading it is much simpler to create read-model classes purposed toward representing queries for specific use cases. In the <a href="http://martinfowler.com/bliki/CQRS.html">CQRS</a> and <a href="http://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> world, persistent read-models or projections are a similar technique for implementing queries.</p>

<h2>Summary</h2>

<ul>
<li>Lazy loading can bring performance benefits in certain use cases.</li>
<li>Lazy loading is not a data loading panacea and can lead to unexpected results.</li>
<li>Lazy loading doesn't scale because it is always restricted by the static nature of the target object model.</li>
<li>Read-models or projections can be used in place of lazy loading.</li>
</ul>

]]></content>
  </entry>
  
</feed>
